
강의 링크 : [https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)


**강의 목표**
- 개발자를 위한 chatGpt 프롬프팅 엔지니어링 강의
- LLMs API 호출을 사용하여 빠르게 소프트웨어 애플리케이션을 구축할 수 있다.
- 소프트웨어 개발을 위한 몇가지 프롬프팅 방법 학습 
- 일반적인 사용 사례, 요약, 추론, 변형, 확장 등을 다룬다
- LLM을 사용하여 챗봇만들기

**base LLM / instruction-turned LLM**
- LLM은 크게 두가지 유형이 있다
  - base LLM
    - 텍스트 트레이닝 데이터를 기반을 다음을 예측하도록 훈련됨
    - 특별한 훈련 없이 대규모 텍스트 데이터셋을 기반으로 훈련된 모델 (언어의 통계적 특성을 학습)
    - 인터넷과 여러 출처 기반으로 많은 양의 데이터를 통해 훈련시텨 다음에 나올 가능성 높은 단어를 파악합니다. 
    - 프랑스의 가장 큰 도시는 무엇인가요? 라는 프롬프터를 주면 인터넷 기사에 따라 프랑스의 가장 큰 도시는 무엇인가요, 프랑스 인구는 얼마나되나요와 같은 흐름의 답변을 할 가능성이 높다.
    - GPT-3가 일반적인 예시 (특정 작업에 대해 별도로 튜닝되지 않은 버전)
  - instruction-turned LLM (명령 조정 언어 모델델)
    - 지시사항에 맞춰 훈련된 LLM
    - 프랑스 수도는 무엇인가요? 질문에 프랑스 수도는 파리입니다. 라고 대답할 가능성이 훨씬 높다. 
    - 대량의 텍스트 데이터로 훈련을 시작하여 지시사항의 입력과 출력 값을 활용해 더욱 fine-tunnning 하고, 종종 RLHF 기법을 통해 더욱 세밀하게 fine-tunning 한다.
    - 시스템에 도임이 되고 지시사항에 따르는 능력을 향상시킨다.
    - ChatGPT, InstructGPT

- instruction-turned LLM은 도움이되고, 정직하며, 혐오 발언이 없는 텍스트에서 훈련이 되었기 때문에 Base LLM에 비해 혐오발언 등의 문제가 될 수 있는 텍스트를 출력하는 확률이 적다. 따라서 현업에서는 instruction-turned LLM를 사용하는 추세이다.
- 해당 강좌에서는 instruction-turned LLM에 초점을 맞쿼 응용 프로그램에 사용해 볼것을 권한다.
- LLM이 가끔씩 작동하지 않을 때는 지식가 명확하지 않기 때문입니다.
  - 질문에 정확히 어디에 초점을 맞춰야하는지 (어떤 분야 측면에서 궁금한것인지), 어떤 어조를 사용하기 원하는지 이런것들이 작업을 수행하는데 도움을 준다.
  - 대학 신입생에게 작업 수행을 요청한다 생각하자. 답변을 하기전 미리 읽어야할 텍스트를 명시해주면 성공적으로 작업을 수행하는데 더욱 도움이 될 것이다. 

**추가 내용**
- 파인튜닝(Fine-tuning)
  - 이미 학습된 모델을 특정 작업이나 데이터에 맞게 추가로 학습시키는 과정
  - 모델이 원하는 답변을 생성하도록 파인튜닝 기법을 통해 출력을 조정하는 방법

- RLHF(Reinforcement Learning from Human Feedback)
  -   파인튜닝을 통해서도 생성된 문장은 똑똑하긴 한데 뭔가 융통성이 부족하다. 때로는 편향적인 발언, 부정적인 정보, 욕설 등을 생성해서 당황스럽게 만들기도 합니다. 전형적으로 사회성이 떨어지는 모델이라고 할 수 있습니다.
  - 모델의 출력에 사람의 선호도를 반영하여 보다 세련된 답변을 할 수 있는 방법이 개발되었고 OpenAI가 ChatGPT를 개발 할 때 적용했던 RLHF가 대표적인 방법중의 하나이다.
  - 사람의 피드백을 모델에 학습하는 기법

- DPO (Direct Preference Optimization)
  - RLHF는 비용이 많이 들고 구성이 복잡하기 때문에 학계나 일반 기업들에게 부담 -> RLHF 보다 간단히고 비용 효율적인 방법들이 고안되었고 DPO는 현재 시점에 가장 많이 사용되는 방식이다. 
  - RLHF와 달리 리워드 모델을 사용하지 않고 모델을 직접 학습하는 방식을 구현


